{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ソーシャルメディアテキストの分析"
      ],
      "metadata": {
        "id": "4pqAdfmDPpaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "-636R2VzPx06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLYscav1PoxM"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "apt-get install mecab libmecab-dev mecab-ipadic-utf8\n",
        "pip install mecab-python3 ipadic nlplot japanize-matplotlib ginza ja-ginza\n",
        "ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "import re\n",
        "import nlplot\n",
        "import MeCab\n",
        "mecab=MeCab.Tagger()\n",
        "import spacy\n",
        "nlp=spacy.load('ja_ginza',disable=['parser','ner']) # 形態素解析だけする設定\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import plotly\n",
        "from plotly.subplots import make_subplots\n",
        "from plotly.offline import iplot\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "-4tpXd2EP5Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ginzaで内容語の抽出\n",
        "def text2bow_ginza(sentence):\n",
        "  words=[]\n",
        "  doc=nlp(sentence)\n",
        "  for token in doc:\n",
        "    if token.pos_ in ['NOUN','VERB','ADJ','ADV','PROPN','PRON']:\n",
        "      words.append(token.lemma_)\n",
        "  return words"
      ],
      "metadata": {
        "id": "7rKXOazslWTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MeCabで内容語の抽出\n",
        "def mecabparse(sentence):\n",
        "  mecab_result=mecab.parse(sentence) # 形態素解析の実行\n",
        "  mecab_result=mecab_result.rstrip() # 最後の改行の削除\n",
        "  out=re.split('\\n',mecab_result) # 改行で分割して1形態素毎のリストにする\n",
        "  return(out)\n",
        "\n",
        "def text2bow_mecab(sentence):\n",
        "  words=[]\n",
        "  morphs=mecabparse(sentence)\n",
        "  for i in range(len(morphs)):\n",
        "    if morphs[i]!='EOS':\n",
        "      line=re.split('\\t',morphs[i])\n",
        "      features=re.split(',',line[1])\n",
        "      if re.match('名詞|動詞|形容詞|副詞',features[0]) and not re.match('非自立|代名詞|接尾',features[1]):\n",
        "        lemma=features[6]\n",
        "        if lemma=='*':\n",
        "          lemma=line[0]\n",
        "        words.append(lemma)\n",
        "  return words"
      ],
      "metadata": {
        "id": "rWSJhEA2P5kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tweetの分析"
      ],
      "metadata": {
        "id": "pMygNlKXhO-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データの読み込み\n",
        "教材に置いてあるstream.20160123_\\{00-16|17-23\\}.tar.bz2をアップロードする．\n",
        "\n",
        "このデータは前処理済みで，\n",
        "```\n",
        "アカウント名 \\t tweet \\t タイムスタンプ\n",
        "```\n",
        "となっている．\n",
        "\n",
        "(各時間ごとにファイルを分割しているので時間単位でデータをまとめる場合はタイムスタンプは見なくても良い)\n"
      ],
      "metadata": {
        "id": "shJ9QfqCCm3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "tar jxvf stream.20160123_00-16.tar.bz2\n",
        "tar jxvf stream.20160123_17-23.tar.bz2"
      ],
      "metadata": {
        "id": "-LG8JGU2gfTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MeCabとGinzaの比較\n",
        "形態素解析だけに限って，MeCabとGinzaの実行速度を見てみる．\n"
      ],
      "metadata": {
        "id": "uhceCtNV_QnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "Hd-VQD5tCb_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/stream.20160123/stream.2016012300.tsv',names=('account','tweet','created_at'),sep='\\t')\n",
        "start=time.time()\n",
        "df['tweet'].head(100).apply(text2bow_mecab)\n",
        "end=time.time()\n",
        "print('MeCab:',end-start)\n",
        "start=time.time()\n",
        "df['tweet'].head(100).apply(text2bow_ginza)\n",
        "end=time.time()\n",
        "print('Ginza:',end-start)"
      ],
      "metadata": {
        "id": "tU9Q_w5P6mb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回はMeCabを使う．"
      ],
      "metadata": {
        "id": "m_uagWj_ARMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=defaultdict(list) # 時間をkey，各ファイルの内容のDataFrameをvalueとしたdict形式にする．\n",
        "for i in range(0,24):\n",
        "  filename='/content/stream.20160123/stream.20160123'+'%02d'%i+'.tsv'\n",
        "  df=pd.read_csv(filename,names=('account','tweet','created_at'),sep='\\t')\n",
        "  df['words']=df['tweet'].apply(text2bow_mecab)\n",
        "  data[i]=df"
      ],
      "metadata": {
        "id": "q3gtYbm2CmFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "各時間ごとのtweetの数を見る．"
      ],
      "metadata": {
        "id": "IeX-Yac6FieN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,24):\n",
        "  print(i,len(data[i]),sep='\\t')"
      ],
      "metadata": {
        "id": "xUI9Dy8zFmrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 各時間帯での頻出語を見る"
      ],
      "metadata": {
        "id": "zpgGJz74ceX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "……前に，まず1時間分だけ見てみる．"
      ],
      "metadata": {
        "id": "M2w2_3U6Lijh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "npt=nlplot.NLPlot(data[0],target_col='words')\n",
        "wc=npt.wordcloud(\n",
        "    max_words=100,\n",
        "    max_font_size=100,\n",
        "    colormap='tab20_r'\n",
        ")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WgwN-MMILleZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データの前処理\n",
        "URLやRTなどが頻度上位に現れるのはあまり嬉しくない．\n",
        "\n",
        "そもそもtweetがどういうテキストでいらなさそうなものが他にあるか見てみる．"
      ],
      "metadata": {
        "id": "eFpkpYzmL83P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0].head(50)"
      ],
      "metadata": {
        "id": "Dq-xuqAXMWMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "どんな前処理をすれば良いかは目的によるが，今回は以下の方針で行う．\n",
        "- URLは削除\n",
        "- RTは\"RT\"だけ削除\n",
        "- \"@アカウント名\"を削除\n",
        "\n",
        "あらためてデータの読み込み．"
      ],
      "metadata": {
        "id": "65hIEehohP-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=defaultdict(list) # 時間をkey，各ファイルの内容のDataFrameをvalueとしたdict形式にする．\n",
        "for i in range(0,24):\n",
        "  filename='/content/stream.20160123/stream.20160123'+'%02d'%i+'.tsv'\n",
        "  account=[]\n",
        "  tweet=[]\n",
        "  timestamp=[]\n",
        "  with open(filename,'r') as fh:\n",
        "    for ln in fh:\n",
        "      ln=ln.rstrip()\n",
        "      line=re.split('\\t',ln)\n",
        "      line[1]=re.sub('https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+','',line[1])\n",
        "      line[1]=re.sub('^RT ','',line[1])\n",
        "      line[1]=re.sub('\\@.+?\\s','',line[1])\n",
        "      if line[1]!='':\n",
        "        account.append(line[0])\n",
        "        tweet.append(line[1])\n",
        "        timestamp.append(line[2])\n",
        "  df=pd.DataFrame({'account':account,'tweet':tweet,'created_at':timestamp})\n",
        "  df['words']=df['tweet'].apply(text2bow_mecab)\n",
        "  data[i]=df"
      ],
      "metadata": {
        "id": "hkuGswDjhrWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "もう一度ワードクラウドを見てみる．"
      ],
      "metadata": {
        "id": "yuhZcshql6hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "npt=nlplot.NLPlot(data[0],target_col='words')\n",
        "wc=npt.wordcloud(\n",
        "    max_words=100,\n",
        "    max_font_size=100,\n",
        "    colormap='tab20_r'\n",
        ")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rXN9tNfyl1l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24時間分見てみる．"
      ],
      "metadata": {
        "id": "9Az-Ajn9mIHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,24):\n",
        "  print(i)\n",
        "  npt=nlplot.NLPlot(data[i],target_col='words')\n",
        "  wc=npt.wordcloud(\n",
        "      max_words=100,\n",
        "      max_font_size=100,\n",
        "      colormap='tab20_r'\n",
        "  )\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.imshow(wc, interpolation=\"bilinear\")\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "bReCua_0HAD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tfidfで見てみる\n",
        "各時間ごとの特徴語を調べるためにtfidfを見てみる．"
      ],
      "metadata": {
        "id": "drz7LuMjrspm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tfとdfの計算\n",
        "tf=defaultdict(lambda:defaultdict(int)) # 各時間あたりの単語の出現頻度\n",
        "df=defaultdict(int) # 単語の文書頻度\n",
        "n_words=defaultdict(int) # 各時間の単語数\n",
        "for i in range(0,24):\n",
        "  tmp=set() # 各時間あたりの単語の出現だけを見るのでlistでなくsetを使う\n",
        "  for words in data[i]['words']:\n",
        "    for w in words:\n",
        "      tf[i][w]+=1\n",
        "      n_words[i]+=1\n",
        "      tmp.add(w)\n",
        "  for t in tmp:\n",
        "    df[t]+=1"
      ],
      "metadata": {
        "id": "kN-UcXs7r9mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "tfidf=defaultdict(float)\n",
        "freq=defaultdict(int)\n",
        "with open('/content/natsume/吾輩は猫である.sentences','r') as fh:\n",
        "  n_w=defaultdict(float) # テキスト中の単語の頻度\n",
        "  n_d=0 # テキスト中の単語数\n",
        "  for ln in fh:\n",
        "    ln=ln.rstrip()\n",
        "    for lemma in getwords(ln):\n",
        "      n_w[lemma]+=1\n",
        "      freq[lemma]+=1\n",
        "      n_d+=1\n",
        "  for w in n_w:\n",
        "    # 頻度が1の単語は除外\n",
        "    if n_w[w]==1:\n",
        "      continue\n",
        "    tf=n_w[w]/n_d\n",
        "    idf=math.log(textnum/df[w])+1\n",
        "    tfidf[w]=tf*idf\n"
      ],
      "metadata": {
        "id": "cNy7PYXmvIkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "tfidf=defaultdict(lambda:defaultdict(float))\n",
        "\n",
        "for i in range(0,24):\n",
        "  for w in tf[i]:\n",
        "    # 全てのテキストに現れる語は0にする\n",
        "    tfidf[i][w]=(tf[i][w]/n_words[i])*(math.log(24/df[w]))"
      ],
      "metadata": {
        "id": "MFm5Y4G2uNiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 上位20件ずつ表示\n",
        "for i in range(0,24):\n",
        "  print('====',i,'====')\n",
        "  srted=sorted(tfidf[i].keys(),key=lambda x:tfidf[i][x],reverse=True)\n",
        "  for j in range(20):\n",
        "    print(srted[j],tfidf[i][srted[j]],sep='\\t')\n"
      ],
      "metadata": {
        "id": "dym6Wfhov1gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 感情分析\n",
        "評判分析のpos/negではなく，いわゆる感情．\n",
        "\n",
        "pymlaskはML-Askのpython実装で，中村の10種類の感情分類に従った辞書ベースの感情推定手法．\n",
        "\n",
        "各時間あたりの感情成文の分布を見てみる"
      ],
      "metadata": {
        "id": "ZiRYYr2tdw5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pymlask"
      ],
      "metadata": {
        "id": "Lbf8jQK3d27_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pymlaskの簡単な使い方説明．"
      ],
      "metadata": {
        "id": "QHxdrH29yJRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlask import MLAsk\n",
        "emotion_analyzer = MLAsk()\n",
        "emolabel={'aware':'哀','iya':'厭','yorokobi':'喜','suki':'好','yasu':'安','takaburi':'昂','odoroki':'驚','ikari':'怒','kowa':'怖','haji':'恥'}"
      ],
      "metadata": {
        "id": "Qe_3CUSmyUMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emo=emotion_analyzer.analyze('京極の小説は大好きだが，本人はちょっと好きではない……')\n",
        "cnt=defaultdict(int)\n",
        "if emo['emotion'] is not None:\n",
        "  for k,v in emo['emotion'].items():\n",
        "    cnt[k]+=1\n",
        "  for k in cnt:\n",
        "    print(emolabel[k],cnt[k],sep='\\t')"
      ],
      "metadata": {
        "id": "lP0-CXb_yNEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweetの感情分析．"
      ],
      "metadata": {
        "id": "6NB6JAZYQWzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emodist=defaultdict(lambda:defaultdict(int))\n",
        "for i in range(0,24):\n",
        "  for t in data[i]['tweet']:\n",
        "    emo=emotion_analyzer.analyze(t)\n",
        "    if emo['emotion'] is not None:\n",
        "      for k,v in emo['emotion'].items():\n",
        "        emodist[i][k]+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "lVwMHJrPfoFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,24):\n",
        "  print('====',i,'====')\n",
        "  for k in emodist[i]:\n",
        "    print(emolabel[k],emodist[i][k])\n"
      ],
      "metadata": {
        "id": "bEsbcCkP1-3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "感情の割合の遷移を折れ線グラフで表示してみる．"
      ],
      "metadata": {
        "id": "kbrE40uL33la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotrans=defaultdict(list) # 各感情の時間ごとの割合を格納\n",
        "for i in range(0,24):\n",
        "  total=0\n",
        "  for k in emodist[i]:\n",
        "    total+=emodist[i][k] # 割合を計算するので総カウント数を求める\n",
        "  for k in emolabel:\n",
        "    if k in emodist[i]:\n",
        "      emotrans[k].append(emodist[i][k]/total)\n",
        "    else:\n",
        "      emotrans[k].append(0.0)"
      ],
      "metadata": {
        "id": "PvstEL4T3_Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in emolabel:\n",
        "  plt.plot(range(0,24),emotrans[k],marker='.',label=k)\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kHqDx2II41Ga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}